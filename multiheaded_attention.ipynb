{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxSw4X64M5tuwm8EVbizk3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinjaljoshi/attention_usage/blob/main/multiheaded_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-Headed Attention** is an extension of self-attention, where instead of applying just one attention mechanism, we use multiple attention mechanisms (heads) in parallel. Each head focuses on different aspects of the input, allowing the model to learn better relationships between words.\n",
        "\n",
        "**Problem with Single-Head Attention**\n",
        "A single self-attention layer only captures one type of relationship between words. For example:\n",
        "\n",
        "It might focus on subject-verb relationships but miss object-adjective dependencies.\n",
        "Solution: Multi-Head Attention\n",
        "\n",
        "Each head learns a different representation of the input.\n",
        "\n",
        "Combining multiple heads helps the model capture different contextual meanings.\n",
        "ðŸ”¹ Example: \"The cat sat on the mat.\"\n",
        "\n",
        "* **Head 1** focuses on word-to-word meaning (cat â†” mat).\n",
        "* **Head 2** focuses on grammatical structure (sat â†” on).\n",
        "* **Head 3** focuses on subject-verb-object relations (cat â†” sat).\n",
        "\n",
        "By combining these different perspectives, the model understands the sentence\n",
        "better.\n",
        "\n",
        "\n",
        "Multi-Head Attention steps\n",
        "1. Input sentence â†’ Convert words to embeddings\n",
        "2. Each head applies self-attention separately (Different projections for Q, K, and V)\n",
        "3. Each head computes its own attention scores and outputs\n",
        "4. Combine all heads into one representation using a linear layer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uixCQaY5Pz3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3FUyuTCvPtig"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example embeddings (word representations)\n",
        "token_embeddings = torch.tensor([\n",
        "    [1.0, 0.5],  # \"The\"\n",
        "    [0.8, 0.2],  # \"Cat\"\n",
        "    [0.9, 0.7],  # \"Sat\"\n",
        "], dtype=torch.float32).unsqueeze(0)  # Shape: (1, 3, 2)  -> (Batch, Seq Length, Embedding Size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, mode=2, num_heads=2):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.mode = mode\n",
        "        assert mode % num_heads == 0, \"mode must be divisible by num_heads\"\n",
        "\n",
        "        self.depth = mode // num_heads  # Size of each headâ€™s Q, K, V\n",
        "\n",
        "        # Learnable weights for Q, K, V for each head\n",
        "        self.W_q = nn.Linear(mode, mode, bias=False)\n",
        "        self.W_k = nn.Linear(mode, mode, bias=False)\n",
        "        self.W_v = nn.Linear(mode, mode, bias=False)\n",
        "\n",
        "        # Final linear layer to combine heads\n",
        "        self.W_o = nn.Linear(mode, mode, bias=False)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Splits the last dimension into (num_heads, depth) and transposes to shape (batch, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, mode = x.shape\n",
        "        x = x.view(batch_size, seq_length, self.num_heads, self.depth)\n",
        "        return x.transpose(1, 2)  # (batch, num_heads, seq_length, depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: Input token embeddings -> Shape: (batch_size, seq_length, mode)\n",
        "        \"\"\"\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Compute Query, Key, Value matrices\n",
        "        Q = self.W_q(x)  # (batch_size, seq_length, mode)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        Q = self.split_heads(Q)  # (batch_size, num_heads, seq_length, depth)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # Compute scaled dot-product attention for each head\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)  # Normalize scores\n",
        "        attention_output = torch.matmul(attention_weights, V)  # Apply attention to values\n",
        "\n",
        "        # Merge heads back into one matrix\n",
        "        attention_output = attention_output.transpose(1, 2).reshape(batch_size, seq_length, self.mode)\n",
        "\n",
        "        # Final linear transformation\n",
        "        output = self.W_o(attention_output)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Initialize multi-head attention with 2 heads\n",
        "multi_head_attention = MultiHeadSelfAttention(mode=2, num_heads=2)\n",
        "\n",
        "# Apply multi-head attention\n",
        "output, attention_weights = multi_head_attention(token_embeddings)\n",
        "\n",
        "print(\"\\nMulti-Head Attention Output:\\n\", output)\n",
        "print(\"\\nAttention Weights:\\n\", attention_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaDlprPnQ2Ce",
        "outputId": "c568d021-1b3e-4e9e-d421-6ee7cb1d0b49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Head Attention Output:\n",
            " tensor([[[0.0865, 0.0770],\n",
            "         [0.0865, 0.0770],\n",
            "         [0.0866, 0.0770]]], grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Attention Weights:\n",
            " tensor([[[[0.3319, 0.3470, 0.3211],\n",
            "          [0.3316, 0.3497, 0.3187],\n",
            "          [0.3328, 0.3388, 0.3284]],\n",
            "\n",
            "         [[0.3398, 0.3332, 0.3270],\n",
            "          [0.3400, 0.3332, 0.3268],\n",
            "          [0.3372, 0.3333, 0.3295]]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}