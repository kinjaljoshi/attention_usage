{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu/iEw0aKOpkDGayt50wE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinjaljoshi/attention_usage/blob/main/masked_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Masked Attention\n",
        "\n",
        "Masked Attention is a variation of self-attention where certain tokens in the sequence are intentionally hidden (masked) so that they do not influence the attention computation. This is primarily used in auto-regressive models where a word should not attend to future words during training.\n",
        "\n",
        "\n",
        "In language generation tasks, we generate words one by one. The model should not look at future words before predicting the next token.\n",
        "\n",
        "**Without masking**\n",
        "The attention mechanism allows each word to see all other words.\n",
        "This is fine for bidirectional models but problematic for autoregressive models.\n",
        "**With masking**\n",
        "The model is forced to only attend to previous words.\n",
        "This ensures it generates text in a left-to-right manner.\n",
        "\n",
        "We apply a mask matrix that prevents the model from attending to future words by replacing future token attention scores with very large negative numbers before applying softmax.\n",
        "\n",
        "**The cat sat on the mat**\n",
        "In masked attention, future words are blocked:\n",
        "\n",
        "\n",
        "* \"The\" sees only itself.\n",
        "* \"Cat\" sees \"The\" and itself.\n",
        "* \"Sat\" sees \"The\" and \"Cat\" but not later words.\n",
        "* Each word can only attend to itself and previous words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MNhU4ts8KcX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, mode):\n",
        "        super().__init__()\n",
        "        self.W_q = nn.Linear(mode, mode, bias=False)\n",
        "        self.W_k = nn.Linear(mode, mode, bias=False)\n",
        "        self.W_v = nn.Linear(mode, mode, bias=False)\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        \"\"\"\n",
        "        token_encodings: (batch_size, seq_length, mode)\n",
        "        \"\"\"\n",
        "        # Compute Query (Q), Key (K), Value (V)\n",
        "        Q = self.W_q(token_encodings)  # (batch_size, seq_length, mode)\n",
        "        K = self.W_k(token_encodings)  # (batch_size, seq_length, mode)\n",
        "        V = self.W_v(token_encodings)  # (batch_size, seq_length, mode)\n",
        "\n",
        "        # Compute similarity scores (QK^T)\n",
        "        similarity_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "        # Scale scores\n",
        "        d_k = K.shape[-1]\n",
        "        scaled_scores = similarity_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "        # Create a mask (upper triangular mask to block future words)\n",
        "        seq_length = token_encodings.shape[1]\n",
        "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)  # Upper triangular matrix\n",
        "        mask = mask.masked_fill(mask == 1, float('-inf'))  # Convert 1s to -inf\n",
        "\n",
        "        # Apply the mask before softmax\n",
        "        masked_scores = scaled_scores + mask.unsqueeze(0)  # Add mask to attention scores\n",
        "\n",
        "        # Apply softmax\n",
        "        attention_weights = F.softmax(masked_scores, dim=-1)\n",
        "\n",
        "        # Compute final attention output\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return attention_output, attention_weights\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example sentence: \"The cat sat\"\n",
        "    token_embeddings = torch.tensor([\n",
        "        [1.0, 0.5],  # \"The\"\n",
        "        [0.8, 0.2],  # \"Cat\"\n",
        "        [0.9, 0.7],  # \"Sat\"\n",
        "    ], dtype=torch.float32).unsqueeze(0)  # Shape: (1, 3, 2)\n",
        "\n",
        "    # Initialize masked self-attention\n",
        "    masked_attention = MaskedSelfAttention(mode=2)\n",
        "\n",
        "    # Apply masked attention\n",
        "    output, attention_weights = masked_attention(token_embeddings)\n",
        "\n",
        "    print(\"\\nFinal Masked Attention Output:\\n\", output)\n",
        "    print(\"\\nMasked Attention Weights:\\n\", attention_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqiVfuMuKZsO",
        "outputId": "0b4c48fb-a3ed-4cc4-f554-1a9267f25cde"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Masked Attention Output:\n",
            " tensor([[[ 0.8301, -0.8463],\n",
            "         [ 0.6860, -0.7036],\n",
            "         [ 0.7573, -0.7715]]], grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "Masked Attention Weights:\n",
            " tensor([[[1.0000, 0.0000, 0.0000],\n",
            "         [0.5003, 0.4997, 0.0000],\n",
            "         [0.3388, 0.3320, 0.3291]]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The → [1.0000, 0.0000, 0.0000]**\n",
        "* \"The\" only attends to itself because it's the first word.\n",
        "\n",
        "**Cat → [0.5003, 0.4997, 0.0000]**\n",
        "* The (50.03%)\n",
        "* Itself (49.97%)\n",
        "\n",
        "**Sat → [0.3388, 0.3320, 0.3291]**\n",
        "* The (33.88%)\n",
        "* Cat (33.20%)\n",
        "* Itself (32.91%)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2IQ9M9CNYV5"
      }
    }
  ]
}